{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "739fbe80-0898-4d60-b183-ca1dfa66d947",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ef50f-2e4f-406c-907a-3b7a60ee4f11",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that empowers computers to understand, interpret, manipulate, and generate human language. This encompasses a wide range of techniques, including text analysis, speech recognition, and machine translation. NLP algorithms strive to decipher the complexities of human communication, such as grammar, semantics, and pragmatics, enabling machines to extract meaning from text, identify sentiment, and even engage in human-like conversations. This technology has profound implications for various sectors, including customer service, healthcare, and education, by automating tasks, improving efficiency, and providing valuable insights from vast amounts of textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4439d-ff7a-42a7-9097-59e80b2757c8",
   "metadata": {},
   "source": [
    "## Usages of NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976147d-39d3-4bce-a550-388d37379754",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) has a wide range of applications across various domains:\n",
    "\n",
    "Customer Service:\n",
    "\n",
    "Chatbots: Powering conversational interfaces for customer support, providing 24/7 assistance and automating routine inquiries.\n",
    "Sentiment Analysis: Analyzing customer feedback (reviews, social media comments) to understand customer sentiment and identify areas for improvement.\n",
    "Search Engines:\n",
    "\n",
    "Understanding User Intent: Interpreting search queries more accurately to deliver relevant and personalized search results.\n",
    "Information Retrieval: Extracting key information from web pages to improve search ranking and provide more comprehensive results.\n",
    "Social Media Analysis:\n",
    "\n",
    "Trend Identification: Monitoring social media conversations to identify emerging trends and understand public opinion.\n",
    "Sentiment Analysis: Analyzing social media posts to gauge public sentiment towards brands, products, and current events.\n",
    "Healthcare:\n",
    "\n",
    "Medical Record Analysis: Extracting key information from patient records to improve diagnosis, treatment planning, and research.\n",
    "Drug Discovery: Analyzing scientific literature to identify potential drug candidates and accelerate drug development.\n",
    "Finance:\n",
    "\n",
    "Financial News Analysis: Analyzing financial news articles and reports to identify market trends and investment opportunities.\n",
    "Fraud Detection: Identifying suspicious activities and patterns in financial transactions through the analysis of textual data.\n",
    "Education:\n",
    "\n",
    "Personalized Learning: Adapting educational content to individual student needs based on their writing style and comprehension level.\n",
    "Automated Grading: Automating the grading process for essays and other assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d6468-6b73-4f78-b6c5-a5896b3f6534",
   "metadata": {},
   "source": [
    "## Usages of NLP in Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebb14c-a242-4af3-aefc-2e33a39b387c",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) plays a crucial role in modern data analytics by unlocking valuable insights from the vast amount of unstructured text data available. Here's how:   \n",
    "\n",
    "Unstructured Data Extraction:\n",
    "\n",
    "NLP techniques extract meaningful information from text sources like social media, customer reviews, news articles, and even internal company communications.   \n",
    "This transforms unstructured data into structured formats that can be analyzed and integrated with other data sources.   \n",
    "Sentiment Analysis:\n",
    "\n",
    "NLP algorithms determine the emotional tone or sentiment expressed in text (positive, negative, neutral).   \n",
    "This is invaluable for understanding customer opinions, market trends, and brand perception.   \n",
    "Topic Modeling:\n",
    "\n",
    "NLP can identify and group similar topics or themes within a large body of text.   \n",
    "This helps in organizing and categorizing information, such as identifying key discussion points in customer support tickets or discovering emerging trends in research papers.   \n",
    "Text Summarization:\n",
    "\n",
    "NLP condenses large volumes of text into concise summaries, saving time and effort for analysts.\n",
    "\n",
    "   \n",
    "This is particularly useful for analyzing lengthy reports, news articles, and legal documents.   \n",
    "Named Entity Recognition (NER):\n",
    "\n",
    "NLP identifies and classifies named entities like people, organizations, locations, and products within text.   \n",
    "This information can be used for market research, competitive analysis, and risk assessment.   \n",
    "By enabling the analysis of textual data, NLP empowers data analysts to:\n",
    "\n",
    "Gain deeper insights: Uncover hidden patterns, trends, and relationships within textual data.   \n",
    "Make better decisions: Inform strategic decisions based on data-driven insights from customer feedback, market trends, and competitor analysis.   \n",
    "Improve efficiency: Automate data analysis tasks, freeing up analysts to focus on higher-level tasks.   \n",
    "Enhance customer experience: Understand customer needs and preferences better, leading to improved products and services.   \n",
    "In essence, NLP bridges the gap between human language and machine understanding, making it an indispensable tool for modern data analytics.   \n",
    "\n",
    "\n",
    "Sources and related content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076af61f-d310-45c3-b3ae-6a644976fa81",
   "metadata": {},
   "source": [
    "## NLP PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33205312-b789-4a9c-ab7b-1bb84414d535",
   "metadata": {},
   "source": [
    "![NLP Pipeline](nlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74866e5e-bf5c-465c-83e3-0edec8ae0ead",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04b26c-9c2f-4007-b531-3af53581ec5c",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens can be individual words, characters, or subwords.   \n",
    "\n",
    "Purpose:\n",
    "\n",
    "Prepare text for analysis: Tokenization is a fundamental step in most NLP tasks. It prepares the text for further processing, such as:\n",
    "Part-of-speech tagging: Identifying the grammatical role of each word (noun, verb, adjective).\n",
    "Named Entity Recognition (NER): Identifying and classifying named entities (people, organizations, locations).\n",
    "Sentiment Analysis: Determining the emotional tone of the text.\n",
    "Machine Translation: Translating text from one language to another.\n",
    "Simplify analysis: By breaking down text into smaller units, it becomes easier for computers to analyze and understand the underlying structure and meaning.\n",
    "Types of Tokenization:\n",
    "\n",
    "Word Tokenization: The most common type, where the text is split into individual words.\n",
    "Example: \"The quick brown fox jumps over the lazy dog.\"\n",
    "Tokens: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "\n",
    "\n",
    "Character Tokenization: The text is broken down into individual characters.\n",
    "Example: \"Hello\"\n",
    "Tokens: [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "\n",
    "\n",
    "Subword Tokenization: The text is split into smaller units than words, such as subwords, prefixes, or suffixes. This is particularly useful for handling rare words or words that are not commonly encountered in the training data.\n",
    "Example: \"running\"\n",
    "Possible subword tokens: \"run\", \"ning\", \"runn\", \"ing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ba426e2-17c8-4e5f-8046-0157de9b8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ashis/nltk_data'\n    - 'C:\\\\Users\\\\ashis\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ashis\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ashis\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ashis\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124mArtificial Intelligence is fascinating! \u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mIt has applications in healthcare, finance, and even entertainment. \u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124mWhat will AI achieve next? Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms explore!\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 3: Tokenize the text into sentences\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# The sent_tokenize function splits the text into a list of sentences.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 3: Sentence Tokenization - Breaking text into sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentences)  \u001b[38;5;66;03m# Output the list of sentences\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ashis/nltk_data'\n    - 'C:\\\\Users\\\\ashis\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ashis\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ashis\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ashis\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Step 1: Download necessary resources\n",
    "# 'punkt' is a pre-trained model for tokenizing text into sentences and words.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Step 2: Define the text to tokenize\n",
    "text = \"\"\"\n",
    "Artificial Intelligence is fascinating! \n",
    "It has applications in healthcare, finance, and even entertainment. \n",
    "What will AI achieve next? Let's explore!\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Tokenize the text into sentences\n",
    "# The sent_tokenize function splits the text into a list of sentences.\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Step 3: Sentence Tokenization - Breaking text into sentences\")\n",
    "print(sentences)  # Output the list of sentences\n",
    "print(\"\\n\")  # Add a line break for readability\n",
    "\n",
    "# Step 4: Tokenize the text into words\n",
    "# The word_tokenize function splits the text into a list of words and punctuation.\n",
    "words = word_tokenize(text)\n",
    "print(\"Step 4: Word Tokenization - Breaking text into words\")\n",
    "print(words)  # Output the list of words\n",
    "print(\"\\n\")\n",
    "\n",
    "# Step 5: Analyze the tokens\n",
    "# Let's count the number of sentences and words\n",
    "num_sentences = len(sentences)\n",
    "num_words = len(words)\n",
    "\n",
    "print(\"Step 5: Token Analysis\")\n",
    "print(f\"Number of sentences: {num_sentences}\")\n",
    "print(f\"Number of words: {num_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d82848-977f-4ca7-8734-fd8164f70af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
